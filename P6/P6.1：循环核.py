#   用RNN实现连续数据的预测
#   循环神经网络：
#   循环核
#   循环核时间步展开
#   循环计算层
#   TF描述循环计算层
#   实践：ABCDE字母预测
#   One-hot
#   Embedding
#   实践：股票预测 :RNN  LSTM GRU

#   卷积核：参数空间共享,卷积层提取空间信息。
#   卷积就是特征提取器,就是CBAPD
#   卷积神经网络：借助卷积核提取空间特征后,送全连接网络实现离散数据的分类。
#   然后有些数据是与时间序列相关的,是可以根据上文预测出下文的。
#   脑具有记忆,记忆体会记录上下文信息,从而可以根据上文信息预测出可能性最大的下文。
#   这种预测就是通过脑记忆体提取历史数据的特征,预测出接下来最可能发生的情况。

#   循环核：类似脑记忆体,具有记忆力通过不同时刻的参数共享,实现了对时间序列的信息提取。循环层提取时间信息。
#   y_{t}
#
#     ↑  w_{hy}
#
#   h_{t}         →  w_{hh}
#
#     ↑  w_{xh}
#
#   x_{t}

#   记忆体,可以设定记忆体的个数,改变记忆容量,当记忆体个数被指定,
#   输入x_{t},输出y_{t}维度被指定,待训练参数w_{xh}、w_{hy}、w_{hh}的维度也就被限定了。
#   记忆体内存储着每个时刻的状态信息h_{t}
#   记忆体当前时刻状态信息 = tanh(当前时刻的输入特征 * 矩阵w_{xh} + 记忆体上一时刻存储的状态信息h_{t-1} * w_{hh} + 偏置想)
#   h_{t} = tanh(x_{t} * w_{xh} + h_{t-1} * w_{hh} + bh)
#   当前时刻循环核的输出特征 = softmax(当前时刻记忆体内存储的状态信息h_{t} * 矩阵w_{hy} + 偏置项 b_{y})
#   y_{t} = softmax(h_{t} * w_{hy} + b_{y}) 本质就是一层全连接
#   前向传播时：记忆体内存储的状态信息h_{t},在每个时刻都被刷新,三个参数矩阵w_{xh},w_{hh},w_{hy}自始至终都是固定不变的。
#   反向传播时：三个参数矩阵w_{xh},w_{hh},w_{hy}被梯度下降法更新。
