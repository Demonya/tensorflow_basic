
#  sigmoid函数：  tf.nn.sigmoid   f\left( x \right) = \frac{1}{1+e^{-x}}
#  tf.nn.sigmoid(x)
#  特点：
#  （1）易造成梯度消失，是因为sigmoid函数的导数输出在（0,0.25），链式法则多次连续相乘趋于0
#  （2）输出非0均值（看函数图像），收敛慢
#  （3）幂运算复杂，训练时间长

#   Tanh函数
#   tf.math.tanh(x)
#   f(x) = \frac{1-e^{-2x}}{1+e^{-2x}}
#   特点：
#   （1）输出是0均值 （2）易造成梯度消失 （3）幂运算复杂，训练时间长。

#   Relu函数   tf.nn.relu(x)
#   f(x) = max(0,x)
#   优点：（1）解决了梯度消失问题（在正区间）（2）只需判断输入是否大于0，计算速度快（3）收敛速度远快于sigmoid和tanh
#   缺点：
#   （1）输出非0均值，收敛慢
#   （2）Dead Relu问题：某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。输入特征为负数是激活函数输出为0，反向传播得到的梯度为0，导致参数无法更新,导致原因主要是输入的特征负数过多。

#   leaky Relu 函数  tf.nn.leaky_relu(x)
#   理论上来讲，Leaky Relu有Relu的所有有点，且不糊有Dead Relu问题，但实际操作中，并没有完全证明Leaky Relu总是好于Relu。

#   初学者建议：
#   （1）首选Relu激活函数（2）学习率设置较小值（3）输入特征标准化，即让输入特征满足以0为均值，1为标准差的正态分布
#   （4）初始参数中心化，即让随机生成的参数满足以0为均值，\sqrt{\frac{2}{当前输入层个数}}为标准差的正态分布。
